---
title: Online evaluation
sidebarTitle: Online evaluation
---

### Evalaute an LLM completion.

When running evaluation on an LLM completion, baserun will automatically create a trace to associate the evaluation with the completion.
So its recommend to use the `baserun.trace` decorator when evaluating completions so each trace will have a proper name.

In the following example, we use the `includes` evaluator as an example.

<CodeGroup>

```python python
import baserun
import openai

@baserun.trace
def example():
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do in Paris?"
            }
        ],
    )
    output = response.choices[0].message.content
    baserun.eval.includes("include_eiffel_tower", output, "Eiffel Tower")
    return output


if __name__ == "__main__":
    baserun.api_key = YOUR_BASERUN_API_KEY_HERE
    openai.api_key = YOUR_OPEANI_API_KEY_HERE

    baserun.init()
    print(example())

```

```typescript typescript
import { baserun } from "baserun";
import OpenAI from "openai";

async function main() {
  await baserun.init();

  const openai = new OpenAI();

  const chatCompletion = await openai.chat.completions.create({
    messages: [
      { role: "user", content: "What are three activities to do in Paris?" },
    ],
    model: "gpt-3.5-turbo",
  });

  const output = chatCompletion.choices[0].message!.content!;
  baserun.eval.includes("include_eiffel_tower", output, ["Eiffel Tower"]);

  // response
  return chatCompletion.choices[0].message.content;
}

main();
```

</CodeGroup>
<Frame>![Online Evaluation](/images/request-with-eval.png)</Frame>

### Evalaute a trace.

You can use evaluation in combination with [Advanced tracing](/monitoring/advanced-tracing) features such as annotation. They allow you to automatically evaluate the output of your code and record the results in the Baserun dashboard alongside completions and custom logs.

When you run an eval within a trace, you will see it displayed in the Trace Details panel.

<CodeGroup>

```python python
import baserun
import openai

def get_activities():
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do on the Moon?"
            }
        ],
    )
    return response.choices[0].message.content

@baserun.trace
def find_best_activity():
    moon_activities = get_activities()
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "Pick the best activity to do on the moon from the following, including a convincing reason to do so.\n + {moon_activities}"
            }
        ],
    )
    output = response.choices[0].message.content
    baserun.eval.includes("include_eiffel_tower", output, "Eiffel Tower")
    return output


if __name__ == "__main__":
    baserun.api_key = YOUR_BASERUN_API_KEY_HERE
    openai.api_key = YOUR_OPEANI_API_KEY_HERE
    baserun.init()
    print(find_best_activity())
```

```typescript typescript
import OpenAI from "openai";
import { baserun } from "baserun";

const openai = new OpenAI();

async function get_activities() {
  const response = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    messages: [
      {
        role: "user",
        content: "What are three activities to do on the Moon?",
      },
    ],
  });
  return response.choices[0].message.content;
}

async function find_best_activity() {
  const moon_activities = await get_activities();

  const response = await openai.chat.completions.create({
    model: "gpt-3.5-turbo",
    temperature: 0.7,
    messages: [
      {
        role: "user",
        content: `Pick the best activity to do on the moon from the following, including a convincing reason to do so.
				${moon_activities.content}`,
      },
    ],
  });

  const output = chatCompletion.choices[0].message!.content!;
  baserun.eval.includes("include_eiffel_tower", output, ["Eiffel Tower"]);

  return response.choices[0].message.content;
}

async function main() {
  await baserun.init({
    apiKey: YOUR_BASERUN_API_KEY_HERE,
  });
  const workflow = baserun.trace(async () => {
    const best_activity = await find_best_activity();
    console.log(best_activity);
    return best_activity;
  }, "Tracing multi-Step");
  await workflow();
}

main();
```

</CodeGroup>
<Frame>![Online Evaluation](/images/trace-with-eval.png)</Frame>
