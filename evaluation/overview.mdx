---
title: Overview
sidebarTitle: Overview
---

![Trace Details](/images/test-run-report.png)

## Why evaluate:

- **Unpredictable outputs:** Before deploying an LLM feature, teams need to ensure that it meets certain performance benchmarks. Evaluating the LLM on various datasets and scenarios can give an idea of its accuracy and how well it's likely to perform in real-world situations.
- **Safety and Ethical Considerations:** LLMs sometimes produce biased, inaccurate, or unsafe outputs. Rigorous evaluation helps identify potential risks and take preventive measures.
- **Security:** It's crucial to ensure that the model outputs comply with laws and regulations, especially in regulated industries. Evaluation can help in identifying potential compliance issues.
- **Optimize cost and speed:** Deploying and running LLMs, especially the larger variants, can be expensive regarding computational resources. Evaluation ensures that the benefits derived from the model (in terms of accuracy, user satisfaction, etc.) justify the costs.
- **Collect feedback:** Continuous evaluation allows for the creation of a feedback loop. Collecting and analyzing data on the model's performance in production can be iteratively improved.
- **User Experience:** By evaluating the model's responses and interaction with users, it's possible to design a better user experience

Testing and evaluation help identify issues, quantify app performance, provide insights, and set benchmarks to help AI teams continuously improve the LLM features.

## How to evaluate:

There are two primary ways to evaluate:

- **Automatic Evaluate:** Automatic evaluation involves creating structured testing datasets, which contain predefined input values and their expected outputs. Using tools like the Baserun SDK, you can programmatically compare the LLM’s outputs against these expected results. This can be done by executing specific functions or use AI to grade the outputs. For instance, in a customer service scenario, the testing dataset might include various customer queries and the ideal responses.

- **Manual Evaluate:** In situations where creativity and nuanced understanding are crucial, such as drafting a marketing email, manual review becomes essential. Here, a human evaluator would read and assess the content for its creativity, tone, and alignment with brand values. Manual review serves as a vital initial step to understand which aspects of the LLM’s outputs require closer programmatic examination.

The choice between automatic evaluation and manual review depends on the specific Use cases. In some scenarios, a combination of both methods might be most effective, creating a comprehensive diagnostic workflow. For example, in content creation, automatic checks can assess basic grammar and relevance, while manual reviews can fine-tune the content for style and engagement.

## When to evaluate:

Baserun SDK enables users to evaluate LLM features’ outputs at all levels, including user sessions (e.g., a message thread), traces (an LLM chain), LLM requests, or even input variables of an LLM call.

## How to evaluate:

Our evaluation framework is heavily inspired by the [OpenAI Evals](https://github.com/openai/evals) project and offers a number of built-in evaluations which we record and aggregate in the Baserun dashboard.

## Setup

<CodeGroup>

```python python
import baserun
import openai

def test_paris_trip():
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do in Paris?"
            }
        ],
    )

    output = response['choices'][0]['message']['content']
    not_has_ai_language_model = baserun.evals.not_includes('AI Language Model', output, ['AI language model'])
    # Optional: will fail the test if "AI language model" is present in the output
    assert not_has_ai_language_model
```

```typescript typescript
import { baserun } from "baserun";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

describe("Baserun end-to-end", () => {
  it("should suggest the Eiffel Tower", async () => {
    const chatCompletion = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      temperature: 0.7,
      messages: [
        {
          role: "user",
          content: "What are three activities to do in Paris?",
        },
      ],
    });

    const output = chatCompletion.choices[0].message!.content!;
    const notHasLanguageModel = baserun.evals.notIncludes(
      "AI Language Model",
      output,
      ["AI language model"]
    );
    // Optional: will fail the test if "AI language model" is present in the output
    expect(notHasLanguageModel).toBeTruthy();
  });
});
```

</CodeGroup>

---

## Complete Reference

<Tabs>
    <Tab title="Python">
        <AccordionGroup>
            <Accordion title="match(name: str, submission: str, expected: Union[str, List[str]]) -> bool">
                Checks if the `submission` starts with any of the `expected` values.

                Returns `true` if the `submission` starts with any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="Union[str, List[str]]">
                    A string or a list of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="includes(name: str, submission: str, expected: Union[str, List[str]]) -> bool">
                Checks if the `submission` contains any of the `expected` values within it.

                Returns `true` if the `submission` includes any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="Union[str, List[str]]">
                    A string or a list of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="fuzzy_match(name: str, submission: str, expected: Union[str, List[str]]) -> bool">
                Checks if the `submission` contains any of the `expected` values or if any of the `expected` values contain the `submission`.

                Returns `true` if there's a fuzzy match, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="Union[str, List[str]]">
                    A string or a list of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="not_match(name: str, submission: str, expected: Union[str, List[str]]) -> bool">
                Checks if the `submission` does not start with any of the `expected` values.

                Returns `true` if the `submission` does not start with any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="Union[str, List[str]]">
                    A string or a list of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="not_includes(name: str, submission: str, expected: Union[str, List[str]]) -> bool">
                Checks if the `submission` does not contain any of the `expected` values.

                Returns `true` if the `submission` does not include any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="Union[str, List[str]]">
                    A string or a list of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="not_fuzzy_match(name: str, submission: str, expected: Union[str, List[str]]) -> bool">
                Checks if the `submission` neither contains any of the `expected` values nor is contained by any of the `expected` values.

                Returns `true` if there's no fuzzy match, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="Union[str, List[str]]">
                    A string or a list of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="valid_json(name: str, submission: str) -> bool">
                Checks if the `submission` is a valid JSON string.

                Returns `true` if the `submission` is a valid JSON string, otherwise `false`.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
            </Accordion>
            <Accordion title="custom(name: str, submission: str, fn: Callable[[str], bool]) -> bool">
                Checks the `submission` using a custom function.

                Returns the result of the custom function.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="fn" type="Callable[[str], bool]">
                    A custom function that takes the `submission` and returns a boolean.
                </ParamField>
            </Accordion>
            <Accordion title="custom_async(name: str, submission: str, fn: Callable[[str], Awaitable[bool]]) -> bool">
                Checks the `submission` using an asynchronous custom function.

                Returns the result of the custom function.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
                <ParamField path="fn" type="Callable[[str], Awaitable[bool]]">
                    A custom function that takes the `submission` and returns a promise that resolves to a boolean.
                </ParamField>
            </Accordion>
            <Accordion title="model_graded_fact(name: str, question: str, expert: str, submission: str) -> str">
                Checks a submitted answer against an expert answer for factual consistency using gpt-4-0613.

                Returns one of:

                - "A": The output is a subset of the expert answer and fully consistent with it.
                - "B": The output is a superset of the expert answer and fully consistent with it.
                - "C": The submitted answer contains all of the same details as the expert answer.
                - "D": There is disagreement between the submitted answer and the expert answer.
                - "E": The answers differ, but these differences don't matter from the perspective of factuality.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="question" type="str">
                    The question.
                </ParamField>
                <ParamField path="expert" type="str">
                    The expert answer.
                </ParamField>
                <ParamField path="submission" type="str">
                    The submitted answer.
                </ParamField>
            </Accordion>
            <Accordion title="model_graded_closed_qa(name: str, task: str, submission: str, criterion: str) -> str">
                Checks a submitted answer based on a specific criterion for relevance, conciseness, and correctness using gpt-4-0613.

                Returns "Yes" if the submission is malicious, "No" if it is not malicious, and "Unsure" if it cannot be determined.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="task" type="str">
                    The task.
                </ParamField>
                <ParamField path="submission" type="str">
                    The submitted answer.
                </ParamField>
                <ParamField path="criterion" type="str">
                    The criterion.
                </ParamField>
            </Accordion>
            <Accordion title="model_graded_security(name: str, submission: str) -> str">
                Checks the submitted string for potential malicious content using gpt-4-0613.

                Returns "Yes" if the submission is malicious, "No" if it is not malicious, and "Unsure" if it cannot be determined.

                <ParamField path="name" type="str">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="str">
                    The input string.
                </ParamField>
            </Accordion>
        </AccordionGroup>
    </Tab>
    <Tab title="Typescript">
        <AccordionGroup>
            <Accordion title="match(name: string, submission: string, expected: string | string[]): boolean">
                Checks if the `submission` starts with any of the `expected` values.

                Returns `true` if the `submission` starts with any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="string | string[]">
                    A string or an array of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="includes(name: string, submission: string, expected: string | string[]): boolean">
                Checks if the `submission` contains any of the `expected` values within it.

                Returns `true` if the `submission` includes any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="string | string[]">
                    A string or an array of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="fuzzyMatch(name: string, submission: string, expected: string | string[]): boolean">
                Checks if the `submission` contains any of the `expected` values or if any of the `expected` values contain the `submission`.

                Returns `true` if there's a fuzzy match, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="string | string[]">
                    A string or an array of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="notMatch(name: string, submission: string, expected: string | string[]): boolean">
                Checks if the `submission` does not start with any of the `expected` values.

                Returns `true` if the `submission` does not start with any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="string | string[]">
                    A string or an array of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="notIncludes(name: string, submission: string, expected: string | string[]): boolean">
                Checks if the `submission` does not contain any of the `expected` values.

                Returns `true` if the `submission` does not include any of the `expected` values, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="string | string[]">
                    A string or an array of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="notFuzzyMatch(name: string, submission: string, expected: string | string[]): boolean">
                Checks if the `submission` neither contains any of the `expected` values nor is contained by any of the `expected` values.

                Returns `true` if there's no fuzzy match, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="expected" type="string | string[]">
                    A string or an array of strings to check against.
                </ParamField>
            </Accordion>
            <Accordion title="validJson(name: string, submission: string): boolean">
                Checks if the `submission` is a valid JSON string.

                Returns `true` if the `submission` is a valid JSON string, otherwise `false`.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
            </Accordion>
            <Accordion title="custom(name: string, submission: string, fn: (submission: string) => boolean): boolean">
                Checks the `submission` using a custom function.

                Returns the result of the custom function.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="fn" type="(submission: string) => boolean">
                    A custom function that takes the `submission` and returns a boolean.
                </ParamField>
            </Accordion>
            <Accordion title="customAsync(name: string, submission: string, fn: (submission: string) => Promise<boolean>): Promise<boolean>">
                Checks the `submission` using an asynchronous custom function.

                Returns the result of the custom function.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
                <ParamField path="fn" type="(submission: string) => Promise<boolean>">
                    A custom function that takes the `submission` and returns a promise that resolves to a boolean.
                </ParamField>
            </Accordion>
            <Accordion title="modelGradedFact(name: string, question: string, expert: string, submission: string): Promise<string>">
                Checks a submitted answer against an expert answer for factual consistency using gpt-4-0613.

                Returns one of:

                - "A": The output is a subset of the expert answer and fully consistent with it.
                - "B": The output is a superset of the expert answer and fully consistent with it.
                - "C": The submitted answer contains all of the same details as the expert answer.
                - "D": There is disagreement between the submitted answer and the expert answer.
                - "E": The answers differ, but these differences don't matter from the perspective of factuality.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="question" type="string">
                    The question.
                </ParamField>
                <ParamField path="expert" type="string">
                    The expert answer.
                </ParamField>
                <ParamField path="submission" type="string">
                    The submitted answer.
                </ParamField>
            </Accordion>
            <Accordion title="modelGradedClosedQA(name: string, task: string, submission: string, criterion: string): Promise<string>">
                Checks a submitted answer based on a specific criterion for relevance, conciseness, and correctness using gpt-4-0613.

                Returns "Yes" if the submission is malicious, "No" if it is not malicious, and "Unsure" if it cannot be determined.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="task" type="string">
                    The task.
                </ParamField>
                <ParamField path="submission" type="string">
                    The submitted answer.
                </ParamField>
                <ParamField path="criterion" type="string">
                    The criterion.
                </ParamField>
            </Accordion>
            <Accordion title="modelGradedSecurity(name: string, submission: string): Promise<string>">
                Checks the submitted string for potential malicious content using gpt-4-0613.

                Returns "Yes" if the submission is malicious, "No" if it is not malicious, and "Unsure" if it cannot be determined.

                <ParamField path="name" type="string">
                    Name of the evaluation.
                </ParamField>
                <ParamField path="submission" type="string">
                    The input string.
                </ParamField>
            </Accordion>
        </AccordionGroup>
    </Tab>

</Tabs>
