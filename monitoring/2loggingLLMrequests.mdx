---
title: Log LLM Requests
sidebarTitle: Log LLM Requests
description: Log LLM Requests including LLM call, Tool call, or any Custom action
---

Throughout the execution of your workflow, each individual step is recorded as a distinct action. This means you can closely observe and track every LLM call, Tool call, or any Custom action as they happen.

LLM request monitoring is used to track the execution of your AI model. These can be used to capture

* Model configuration like model name, model configurations, prompt templates, etc.
* Execution details such as prompt tokens, completion tokens, cost, etc.
* API-level metrics like request latency, rate limit errors, etc.
* OpenAI tool calls in JSON format

![Monitoring_requests](/images/monitor-logging-requests.png)

## Use cases

For example, you are creating a AI travel agent to assist with booking trips. This bot needs to accurately gather flight ticket details from scrapping user emails. With logging LLM Request, you can:

1. Track and Fix Errors: Test the same task repeatedly to see how often and where your bot throws an errors. 
2. Ensure Quality: Verify that your bot is correctly identifying flight prices, dates, and booking details. You can check if the bot is giving the user right response based on the email input. 
3. Improve User Experience: Keep track of the response time for each request. This helps find any delays in the bot's responses, leading to improvement in how users interact with the bot.

## Features 
* Automatically logs OpenAI and Anthropic LLM calls
* Support for Python and Typescript
* Provides generative token usage and context token usage
* Provides estimated cost and duration
* Supports async functions
* Supports automatic checks
* Supports collecting user feedback

Instructions

If you haven't configured Baserun, please refer to the getting started section for setup instructions.

All you need to do is install the Baserun SDK, define the environment in which you'd like to run Baserun, and initialize it at your application's startup. 
<CodeGroup>
```python python
import baserun
import openai


def example():
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do in Paris?"
            }
        ],
    )
    return response.choices[0].message


if __name__ == "__main__":
    baserun.api_key = YOUR_BASERUN_API_KEY_HERE
    openai.api_key = YOUR_OPEANI_API_KEY_HERE
    baserun.init()
    print(example())
```
```typescript typescript
import {baserun} from 'baserun'; 
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: YOUR_OPEANI_API_KEY_HERE, 
});


async function example() {
  const chatCompletion = await openai.chat.completions.create({
    messages: [{ role: 'user', content: 'What are three activities to do in Paris?' }],
    model: 'gpt-3.5-turbo',
  });
  return chatCompletion.choices[0].message.content;
}


baserun.init();
example().then(output => {
    console.log("Output:", output);
  }).catch(error => {
    console.error("Error:", error);
  });
````
</CodeGroup>
