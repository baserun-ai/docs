---
title: "Collect user feedback"
sidebarTitle: "Collect user feedback"
---

In addition to automatic evaluations and human evaluations, user feedback is an excellent source for assessing the quality of an LLM application. In Baserun, feedback is collected either as a 0-1 score or in the form of comments and is attached to a trace or an individual LLM request.

## Use case

- Evaluate chatbot response: You might want to ask user rate the chatbot response with a ‚Äùüëç‚Äù / ‚Äùüëé‚Äù or on a numeric scale (eg: ‚Äú1-5‚Äù) as a way to evaluate customer experience.
- Create fine-tuning datasets: You might want to collecting positive responses use for fine-tuning your model.
- Create benchmark testing datasets: Consider collecting negative responses to identify use cases that were not covered in previous development cycles. These user requests can then be added to regression testing suites to evaluate the performance of your next release.

## Features

- Collect score rating
- Collect comments
- Support user feedback on traces and LLM requests

User feedback is attached to an LLM request or a trace. Please start by following 'Get Started with Logging LLM Requests' or 'Get Started with Tracing a Workflow' first. Once you have successfully logged LLM requests or traces in Baserun, continue with the following instructions.

## Instructions

You can customize your UI in different ways to collect user feedback. Whether it's through a 'üëç'/'üëé' or on a numeric scale (e.g., '1-5'), the customer's score will be displayed as a 0-1 score on the Baserun dashboard. The end user also has the option to add a comment.

User feedback is a type of annotation, together with checks, automatic evaluations and human evaluations.
In order to create an Annotation, you use the `baserun.annotate` function:

<CodeGroup>
```python
@baserun.trace
def ask_question(question="What is the capital of the US?") -> str:
    completion = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[{"role": "user", "content": question}],
    )

    # Create the annotation
    annotation = baserun.annotate()

    # Capture whatever annotations you need
    annotation.feedback(
        name="annotate_feedback", score=0.8, metadata={"comment": "This is correct but not concise enough"}
    )
    annotation.check_includes("openai_chat.content", "Washington", content)
    annotation.log("OpenAI Chat Results", metadata={"result": content, "input": question})

    # Make sure to submit the annotation
    annotation.submit()

````
</CodeGroup>

To associate these annotations a particular LLM request, you simply need to pass the completion ID from your LLM request. To do so using OpenAI's SDK, you can do the following:

<CodeGroup>
```python
@baserun.trace
def ask_question(question="What is the capital of the US?") -> str:
    completion = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[{"role": "user", "content": question}],
    )

    # Create the annotation
    annotation = baserun.annotate(completion.id)

    # Capture whatever annotations you need
    annotation.feedback(
        name="annotate_feedback", score=0.8, metadata={"comment": "This is correct but not concise enough"}
    )
    annotation.check_includes("openai_chat.content", "Washington", content)
    annotation.log("OpenAI Chat Results", metadata={"result": content, "input": question})

    # Make sure to submit the annotation
    annotation.submit()
````

</CodeGroup>

Here is how the user feedback will look like in Baserun dashboard:
![User feedback](/images/eval_user_feedback.png)
