---
title: Advanced tracing features
sidebarTitle: Advanced tracing features
description: Tracing additional data such as trace name, custom log, session ID, etc
---

This page goes over some advanced tracing features.

## Adding a Trace Name

If you're tracing multiple functions, you can use the `start_trace` context manager to distinguish between them:

<CodeGroup>
```python
def ask_question(question="What is the capital of the US?") -> str:
    with baserun.start_trace(name="General knowledge question"):
        # Your code here
```
</CodeGroup>

Within a `start_trace` context any calls to OpenAI or Anthropic will automatically be included in the trace.

## Setting a trace's result

By default a trace's `result` value will be the return value of the function that is traced. If you want to be more explicit, you can set the `result` value of a trace.

<CodeGroup>
```python
def ask_question(question="What is the capital of the US?") -> str:
    with baserun.start_trace() as trace:
        # Your code here

        trace.result = my_result
```
</CodeGroup>

## Adding custom metadata

You can add custom metadata to a trace as well. This metadata could be whatever you like, as long as it's JSON serializable. For instance, you may want to include references to other objects or systems.

<CodeGroup>
```python
def ask_question(question="What is the capital of the US?") -> str:
    with baserun.start_trace() as trace:
        # Your code here

        # Add whatever metadata you like
        trace.metadata = {
          "answer_id": answer_id
        }
```
</CodeGroup>

## Capturing Annotations

Annotations are a term we use to refer to events and metadata that can be attached to a trace or an individual LLM request. Examples are logs, user feedback, evals, and checks.

To associate these annotations with a trace, you need to create an Annotation.

In order to create an Annotation, you use the `baserun.annotate` function:

<CodeGroup>
```python
@baserun.trace
def ask_question(question="What is the capital of the US?") -> str:
    completion = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[{"role": "user", "content": question}],
    )

    # Create the annotation
    annotation = baserun.annotate()

    # Capture whatever annotations you need
    annotation.feedback(
        name="annotate_feedback", score=0.8, metadata={"comment": "This is correct but not concise enough"}
    )
    annotation.check_includes("openai_chat.content", "Washington", content)
    annotation.log("OpenAI Chat Results", metadata={"result": content, "input": question})

    # Make sure to submit the annotation
    annotation.submit()
```
</CodeGroup>

To associate these annotations a particular LLM request, you simply need to pass the completion ID from your LLM request. To do so using OpenAI's SDK, you can do the following:

<CodeGroup>
```python
@baserun.trace
def ask_question(question="What is the capital of the US?") -> str:
    completion = client.chat.completions.create(
        model="gpt-4-1106-preview",
        messages=[{"role": "user", "content": question}],
    )

    # Create the annotation
    annotation = baserun.annotate(completion.id)

    # Capture whatever annotations you need
    annotation.feedback(
        name="annotate_feedback", score=0.8, metadata={"comment": "This is correct but not concise enough"}
    )
    annotation.check_includes("openai_chat.content", "Washington", content)
    annotation.log("OpenAI Chat Results", metadata={"result": content, "input": question})

    # Make sure to submit the annotation
    annotation.submit()
```
</CodeGroup>