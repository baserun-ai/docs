---
title: Trace multi-step workflows
sidebarTitle: Trace multi-step LLM workflows
description: Log LLM Requests including LLM call, Tool call, or any Custom action
---

## Introduction

A Trace comprises a series of events executed within an LLM chain. Tracing enables Baserun to capture and display the LLM chain's entire lifecycle, whether synchronous or asynchronous.

Tracing LLM chains allows you to debug your application, monitor your LLM chains' performance, and collect customer feedback.

For example, consider the case of an AI bot used to automate phone calls. The process begins with the bot initiating a call to the user. While the call is in progress, the conversation is transcribed into text. Subsequently, the bot analyzes the transcribed text to produce a response or message. Once the response is crafted, it is converted into audio. Finally, the AI system transmits the audio message.

![Trace a LLM chain](/images/monitor-trace-details.png)

## Use case

Please reference the [Monitoring Overview](/monitoring/overview) to learn why logging LLM chain is critical for LLM feature development.

## Features

- Support Python and Typescript
- Automatically logs OpenAI and Anthropic LLM calls
- UI to show sequence of events
- Provide generative token usage, context token usage, estimated cost, duration
- Support automatic evaluation
- Supports async functions
- Option to add custom trace name
- Option to log custom metadata
- Option to set trace result
- Support collecting user feedback

<Note>
  If you are using Next.js, please reference [Logging > Tracing with
  Next.js](/logging/tracingwithnextjs) .
</Note>

## Instruction

If you haven't configured Baserun, please refer to the getting started section for setup instructions.

Add @baserun.trace to the function you want to trace. In the following example, our workflow constitutes two LLM calls.

1. List out three activities to do on the moon
2. Choose the best activity among the three

## Example

```python python
import baserun
import openai
import sys


def get_activities():
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do on the Moon?"
            }
        ],
    )
    command = " ".join(sys.argv)
    return response.choices[0].message

@baserun.trace
def find_best_activity():
    moon_activities = get_activities()
    response = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": f"Pick the best activity to do on the moon from the following, including a convincing reason to do so.\n + {moon_activities}"
            }
        ],
    )
    return response.choices[0].message


if __name__ == "__main__":
    baserun.api_key = YOUR_BASERUN_API_KEY_HERE
    openai.api_key = YOUR_OPEANI_API_KEY_HERE
    baserun.init()
    print(find_best_activity())
```
