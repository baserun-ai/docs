---
title: Tracing with Next.js
sidebarTitle: Tracing with Next.js
description: Start tracing your Next.js app.
---

<Steps>
<Step title="Install Baserun SDK">

<CodeGroup>

    ```bash typescript
    npm install baserun
    # or
    yarn add baserun
    ```

</CodeGroup>
</Step>
<Step title="Generate an API key">

Create an account at [https://baserun.ai](https://baserun.ai). Then generate an API key for your project in the [settings](https://baserun.ai/settings) tab and set it as an environment variable.

```bash
export BASERUN_API_KEY="your_api_key_here"
```

</Step>
<Step title="Initialize baserun">

Use the [Next.js instrumentation hook](https://nextjs.org/docs/app/building-your-application/optimizing/instrumentation) to ensure baserun is initialized exactly once during server startup.

```typescript your-project/instrumentation.ts
import { baserun } from 'baserun'

export async function register() {
  // Baserun is not yet supported in the edge runtime
  if (
    process.env.NODE_ENV === 'production' &&
    process.env.NEXT_RUNTIME === 'nodejs'
  ) {
    await baserun.init()
  }
}
```

</Step>
<Step title="Update next.config.js">

Add the following to your next.config.js to ensure the instrumentation hook is used and that we can automatically log LLM calls.

```javascript javascript
/** @type {import('next').NextConfig} */

module.exports = {
  experimental: {
    instrumentationHook: true,
    // Libraries you want to automatically log
    // Options are: 'openai', 'openai-edge', and '@anthropic-ai/sdk'
    serverComponentsExternalPackages: ['openai'],
  },
}
```

</Step>
<Step title="Add a trace">

Add a trace to start logging to Baserun.

```typescript typescript
import { baserun } from 'baserun'

const traceGetResponse = baserun.trace(aiService.getResponse, { userId: 123 })
const response = await traceGetResponse(message)
```

</Step>

</Steps>

## Example

In this example, we're using Vercel's [ai](https://www.npmjs.com/package/ai) npm package to stream the result of an OpenAI chat completion to the client.
Note, that Baserun doesn't support `export const runtime = 'edge';`, we we're keeping this in an ordinary Lambda function deployed by Vercel.

```javascript ./app/api/chat/route.js
import OpenAI from 'openai'
import { OpenAIStream, StreamingTextResponse } from 'ai'
import { baserun } from 'baserun'

const openai = new OpenAI()

export const POST = baserun.trace(async (req) => {
  const { messages } = await req.json()
  const response = await openai.chat.completions.create({
    model: 'gpt-4',
    stream: true,
    messages,
  })
  const stream = OpenAIStream(response)
  return new StreamingTextResponse(stream)
}, 'chat route')
```
