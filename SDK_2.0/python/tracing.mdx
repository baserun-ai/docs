---
title: "Tracing"
sidebarTitle: "Tracing"
---

## Logging LLM requests

Get insight into individual LLM request, including prompt templates, input, output, duration, token usage cost, etc. Collect datasets for continuous iterations (fine-tuning, testing).

<Frame>![Logging LLM requests](/images/2.0-python-llm_request.gif)</Frame>

An <code>LLM request</code> represents a single query to an LLM provider. Baserun refers to the returned object as a <code>Completion</code>.
If the request is successful, Baserun logs the completion in the UI as shown above. A completion includes the input and output of the request, along with metadata such as the user, request ID, and model configurations.
If the request fails, Baserun logs the error code and message in the LLM requests table.

In addition to the underlying library's default arguments when creating a completion, users have the flexibility to define the following custom arguments to enhance the analysis and data collection experience.

### Arguments

These properties can be passed, in addition to the normal arguments passed to your client, as keyword arguments when creating an LLM request (e.g. when using `completions.create`). They can also be set on the completions object returned from the client.

```python
completion = client.chat.completions.create(
    name="Paris Activities", # optional name
    model="gpt-4o",
    temperature=0.7,
    messages=[
        {
            "role": "user",
            "content": "What are three activities to do in Paris?"
        }
    ],
)
```

<Accordion title="client.chat.completions.create(name: Optional[string], user: Optional[string], session: Optional[string], metadata: optional[Dict]): Completion">
    <ParamField path="name" type="string">
      Custom name of the LLM request
    </ParamField>

    <ParamField path="completion_id" type="UUID">
      Created by Baserun but can be overridden by the user.
    </ParamField>

    <ParamField path="user" type="string">
      Inherited from the parent trace if there is one, unless a `user` argument is passed when creating the completion.
    </ParamField>
</Accordion>

These arguments also apply to other completion-generating functions such as `stream`.

### Tagging functions

These functions can be called on the completion object to add tags to the completion. Tags are key-value pairs that can be used to add extra information to a completion.


<Accordion title="completion.tag(key: string, value: string, metadata: Dict, tag_type: string): Tag">
    <ParamField path="key" type="string" required />
    <ParamField path="value" type="string/numeric/boolean/object/array" required />
    <ParamField path="metadata" type="object" />
    <ParamField path="tag_type" type="string">
      The type of tag. Default tag types are `tag`, `log`, `variable`, and `feedback`.
    </ParamField>
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

There are also convenience functions for the built-in tag types:

<Accordion title="completion.log(message: string, name: Optional[string] = None, metadata: Optional[Dict] = None): Tag">
    <ParamField path="message" type="string" required />
    <ParamField path="name" type="string">An optional name that will show up in the trace detail view in the UI</ParamField>
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

<Accordion title="completion.variable(key: string, value: string, metadata: Optional[Dict] = None): Tag">
    <ParamField path="key" type="string" required />
    <ParamField path="value" type="string" required />
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

<Accordion title="completion.feedback(name: string, score: float, metadata: Optional[Dict] = None): Tag">
    <ParamField path="name" type="string">An optional name that will show up in the trace detail view in the UI</ParamField>
    <ParamField path="score" type="float" required >A score between 0 and 1</ParamField>
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>


### Adding tags to a completed trace or completion

After a trace has been completed you may wish to add additional tags to a trace or completion. For example, you might have user feedback that is gathered well after the fact. To add these tags, you need to store the <code>trace_id</code>, and, if the tag is for a completion, the `completion_id`. You can then use the <code>tag</code>, <code>log</code>, or <code>feedback</code> functions to submit those tags. These functions are equivalent to the functions on the `Completion` object, but they take the `trace_id` and `completion_id` as arguments.

```python
from baserun import OpenAI, log, feedback

client = OpenAI(name="trace to be resumed")
completion = client.chat.completions.create(
    name="completion to be resumed",
    model="gpt-4o",
    messages=[{"role": "user", "content": "What are three activities to do in Paris?"}],
)

# Store these values
trace_id = client.trace_id
completion_id = completion.completion_id

# A few moments later...
log("Tagging resumed", trace_id=trace_id, completion_id=completion_id)
feedback("User satisfaction", 0.9, trace_id=trace_id, completion_id=completion_id)
```

<Accordion title="log(message: string, name: Optional[string] = None, metadata: Optional[Dict] = None): Tag">
    <ParamField path="message" type="string" required />
    <ParamField path="name" type="string">An optional name that will show up in the trace detail view in the UI</ParamField>
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

<Accordion title="variable(key: string, value: string, metadata: Optional[Dict] = None): Tag">
    <ParamField path="key" type="string" required />
    <ParamField path="value" type="string" required />
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

<Accordion title="feedback(name: string, score: float, metadata: Optional[Dict] = None): Tag">
    <ParamField path="name" type="string">An optional name that will show up in the trace detail view in the UI</ParamField>
    <ParamField path="score" type="float" required >A score between 0 and 1</ParamField>
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

### Instructions

<Steps>
<Step title="Install Baserun SDK">
```bash
pip install baserun
```
</Step>
<Step title="Set the Baserun API key">

Create an account at [https://app.baserun.ai/sign-up](https://app.baserun.ai/sign-up). Then generate an API key for your project in the [settings](https://app.baserun.ai/settings) tab. Set it as an environment variable:

```bash
export BASERUN_API_KEY="your_api_key_here"
```

</Step>
<Step title="Import and Init">
In order to have Baserun trace your LLM Requests, all you need to do is import <code>OpenAI</code> from <code>baserun</code> instead of <code>openAI</code>. Creating an <code>OpenAI</code> client object automatically starts the trace, and all future LLM requests made with this client object will be captured.

```python
from baserun import OpenAI


def example():
    client = OpenAI()
    completion = client.chat.completions.create(
        name="Paris Activities", # optional name
        model="gpt-4o",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do in Paris?"
            }
        ],
    )


if __name__ == "__main__":
    print(example())

```

</Step>
<Step title="Alternate init method">
If you don't wish to use Baserun's OpenAI client, you can simply wrap your normal OpenAI client using init.

```python
from baserun import init

client = init(OpenAI())
completion = client.chat.completions.create(
    ...
)
```

</Step>
</Steps>

## Tracing end-to-end pipelines

A **Trace** comprises a series of events executed within an LLM chain (also called a workflow, among other names). Tracing enables Baserun to capture and display the LLM chain's entire lifecycle, whether synchronous or asynchronous.

Using Baserun, traces are tied to the client object of the library you are using. For example, if you are using the OpenAI library, you would create an `OpenAI` client object imported from `baserun`. When that client object is used all completions are automatically traced.

### Arguments

These arguments can be passed when instantiating your client object, or can be set after instantiation.

<Accordion title="OpenAI(name: string, **kwargs): OpenAI">

    <ParamField path="name" type="string">
      Custom name of the LLM request
    </ParamField>

    <ParamField path="completion_id" type="UUID">
      Created by Baserun but can be overridden by the user.
    </ParamField>

    <ParamField path="trace_id" type="UUID">
      Created by Baserun but can be overridden by the user.
    </ParamField>

    <ParamField path="user" type="string">
      An identifier for the end user. Can be any string, e.g. a user ID or email.
    </ParamField>

    <ParamField path="session" type="string">
      An identifier for the current session. For example, you might user a session ID from your application framework or authenticiation library.
    </ParamField>

    <ParamField path="result" type="string">
      The output or outcome of the trace, will be shown in the "Output" field in the Baserun UI.
    </ParamField>

    <ParamField path="metadata" type="object" />

    <br />
    **Return type**
    <ResponseField type="OpenAI" name="OpenAI client object" />
</Accordion>

### Instructions

In the following example, this pipeline has two LLM calls. Create a client at the beginning of the function you want to trace, and pass the client anywhere you want to use the same trace.

```python
from baserun import OpenAI

def get_activities(client: OpenAI):
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do on the Moon?"
            }
        ],
    )
    return response.choices[0].message

def find_best_activity():
    client = OpenAI()
    client.name = "find_best_activity"
    client.user = "user123"
    client.session = "session123"

    moon_activities = get_activities(client)
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "Pick the best activity to do on the moon from the following, including a convincing reason to do so.\n + {moon_activities}"
            }
        ],
    )
    client.result = "success"
    return response.choices[0].message
```

Alternatively, you can associate two events with the same trace or resume a trace using the <code>trace_id</code>. If you wish to associate an LLM request with a trace after the trace has completed, see the example below. Another common use case is when you want to add user feedback or tags to a trace after the pipeline has finished executing.

```python
from baserun import OpenAI

def main():
    main_trace_id = str(uuid4())
    activities = get_activities(main_trace_id)
    find_best_activity(main_trace_id, activities)


def get_activities(trace_id: str) -> str:
    client = OpenAI(trace_id=trace_id)
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "What are three activities to do on the Moon?"
            }
        ],
    )
    return response.choices[0].message

def find_best_activity(trace_id: str, activities: str) -> str:
    client = OpenAI(trace_id=trace_id)
    client.name = "find_best_activity"
    response = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.7,
        messages=[
            {
                "role": "user",
                "content": "Pick the best activity to do on the moon from the following, including a convincing reason to do so.\n + {activities}"
            }
        ],
    )
    client.result = "success"
    return response.choices[0].message
```

### Supported Models

At the moment, the Baserun Python SDK 2.0 supports all models using OpenAI library and Anthropic library, regardless of the underlying model's provider. We are continuously adding support for new models. If you have a specific model you would like to use, please reach out to us at [hello@baserun.ai](mailto:hello@baserun.ai) or [join our community](https://discord.gg/EtR7NFQ4).

If you use another provider or library, you can still use Baserun by manually creating "generic" objects. Notably, generic completions must be submitted explicitly using `submit_to_baserun()`. Here's what that looks like:


```python
from baserun.wrappers.generic import (
    GenericChoice,
    GenericClient,
    GenericCompletion,
    GenericCompletionMessage,
    GenericInputMessage,
)

question = "What is the capital of the US?"
response = call_my_custom_model(question)

client = GenericClient(name="My Traced Client")
completion = GenericCompletion(
    model="my custom model",
    name="My Completion",
    input_messages=[GenericInputMessage(content=question, role="user")],
    choices=[GenericChoice(message=GenericCompletionMessage(content=response))],
    client=client,
    trace_id=client.trace_id,
)
completion.submit_to_baserun()
```

### Tagging functions

Much like the completion object, these functions can be called on the trace object to add tags to the trace. Tags are key-value pairs that can be used to add extra information to a trace.


<Accordion title="client.tag(key: string, value: string, metadata: Dict, tag_type: string): Tag">
    <ParamField path="key" type="string" required />
    <ParamField path="value" type="string/numeric/boolean/object/array" required />
    <ParamField path="metadata" type="object" />
    <ParamField path="tag_type" type="string">
      The type of tag. Default tag types are `tag`, `log`, `variable`, and `feedback`.
    </ParamField>
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

There are also convenience functions for the built-in tag types:

<Accordion title="client.log(message: string, name: Optional[string] = None, metadata: Optional[Dict] = None): Tag">
    <ParamField path="message" type="string" required />
    <ParamField path="name" type="string">An optional name that will show up in the trace detail view in the UI</ParamField>
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

<Accordion title="client.variable(key: string, value: string, metadata: Optional[Dict] = None): Tag">
    <ParamField path="key" type="string" required />
    <ParamField path="value" type="string" required />
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>

<Accordion title="client.feedback(name: string, score: float, metadata: Optional[Dict] = None): Tag">
    <ParamField path="name" type="string">An optional name that will show up in the trace detail view in the UI</ParamField>
    <ParamField path="score" type="float" required >A score between 0 and 1</ParamField>
    <ParamField path="metadata" type="object" />
    <br />
    **Return type**
    <ResponseField type="Tag" name="tag" />
</Accordion>