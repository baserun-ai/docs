---
title: Tracing
description: Monitor your LLM workflows in production.
---

Tracing behaves very similar to testing but is intended for use with your production application. When tracing a function we automatically aggregate all of your [LLM and custom logs](/logging) in that function's call stack.

## What is a trace?

A trace is used to group together LLM calls performed within a function. This allows Baserun to show you the whole lifecycle of a function, including all of the LLM calls and custom logs that were generated.

A Baserun trace is very similar to a trace in observability frameworks, except that only LLM calls and custom logs are captured.

## What to trace?

The function(s) to trace are ultimately dependent on your app. It could be a `main()` function, or it could be a handler for an API call.

As an example, in the [Python example agent](https://github.com/baserun-ai/testing-agent/blob/main/demo/main.py#L39), the `main()` function is traced. In the [Typescript example agent](https://github.com/baserun-ai/testing-agent-js/blob/main/src/main.ts#L14) we trace the agent's `run` function by wrapping it.

## How to trace

In order to trace in production you must first `init` baserun. This should be done at application startup.

<CodeGroup>
```python python
import baserun

if env == 'production':
    baserun.init()
```

```typescript typescript
import { baserun } from 'baserun';

if (process.env.NODE_ENV === 'production') {
    baserun.init();
}
```

</CodeGroup>

Then, set up tracing for the function(s) you would like to trace.

<CodeGroup>
```python python
# Decorate the function that you would like to trace:
import baserun

@baserun.trace
def get_response(message):
    ...

```

```typescript typescript
import { baserun } from 'baserun';

const traceGetResponse = baserun.trace(aiService.getResponse, { userId: 123 });
const response = await traceGetResponse(message);
````

</CodeGroup>

## Tracing in the Baserun UI

In the Baserun UI, you can visit the "Monitoring" page to view your traces. This shows a list of each trace and the result of the traced function.

![Trace List](/images/trace-example.png)

To view the logs for a trace, click on the trace. This will show you the logs for each LLM call in the trace. This allows you to see the full context of each LLM call.

![Trace Details](/images/trace-details.png)

## Guides

For most setups the above example should be sufficient. For more involved implementations, we have guides for some of the most popular frameworks like [Next.js](/nextjs).

## API Reference

### trace

Aggregates a trace of all logs generated by a function call to Baserun. If Baserun is not initialized, baserun.trace will have no effect.

<ParamField path="function" type="Function">
  The function to be traced.
</ParamField>
<ParamField path="metadata" type="object, optional">
  Additional metadata to record alongside the traced logs.
</ParamField>

## Special Note: Python Threads

If you start a trace or test and need to spawn a thread, Baserun's tracing context must be propagated to the new thread. To do this, wrap your target function using `baserun.thread_wrapper`. For example:

```python
@baserun.trace
def answer_question_in_thread(question: str):
    my_thread = Thread(
        target=baserun.thread_wrapper(openai_chat),
        args=(question,),
    )
    my_thread.join()

def openai_chat(question: str) -> str:
    completion = ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
    )
    return completion.choices[0]["message"].content


answer_question_in_thread("What is the capital of the state of California?")
```

This allows calls to `openai_chat` to be associated with the traced function that spawned the thread.
