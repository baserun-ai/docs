---
title: Tracing
description: Monitor your LLM workflows in production.
---

Tracing behaves very similar to testing but is intended for use with your production application. When tracing a function we automatically aggregate all of your [LLM and custom logs](/logging) in that function's callstack.

## Setup

Add your BASERUN_API_KEY as an environment variable and then initialize Baserun at the entry point of your application.

<CodeGroup>
```python python
import baserun

if env == 'production':
    baserun.init()
```

```typescript typescript
import { baserun } from 'baserun';

if (process.env.NODE_ENV === 'production') {
    baserun.init();
}
```
</CodeGroup>

Trace a function
<CodeGroup>
```python python
# Decorate the function that you would like to trace:
import baserun

@baserun.trace
def get_response():
    ...

# Or wrap the function and optionally pass any additional metadata:
def get_response(message):
    ...

traced_get_response = baserun.trace(get_response, {"userId": 123})
traced_get_response(message)
```

```typescript typescript
import { baserun } from 'baserun';

const traceGetResponse = baserun.trace(aiService.getResponse, { userId: 123 });
const response = await traceGetResponse(message);
```
</CodeGroup>

### trace

Aggregates a trace of all logs generated by a function call to Baserun. If Baserun is not initialized, baserun.trace will have no effect.

<ParamField path="function" type="Function">
    The function to be traced.
</ParamField>
<ParamField path="metadata" type="object, optional">
    Additional metadata to record alongside the traced logs.
</ParamField>