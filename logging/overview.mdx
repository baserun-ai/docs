---
title: Logging overview
sidebarTitle: Overview
description: Logging helps you track the performance of your application, identify issues, and understand the application’s behavior.
---

This guide will introduce you to the concept of logging in Baserun, including how it works and how logs can be used to monitor, debug, and optimize the performance of your AI application.​

## Why log your LLM Chains?

1. **In-depth Analysis:** Logging provides detailed information about each event in your AI model’s life cycle, allowing you to understand how your model or pipeline is operating, where it’s spending most of its time, and where potential issues or bottlenecks might be.
2. **Debugging and Optimization:** With the detailed view provided by logging, you can easily identify any errors or performance issues. This makes debugging easier and allows you to optimize your model or pipeline for better performance.
3. **Monitoring:** Regularly checking the logs can help you spot any unusual behavior or performance issues early on, allowing you to proactively address potential problems.
4. **Dataset Curation:** Based on user feedback and metric performance, you can use logging to curate your evaluation and fine-tuning datasets to improve the quality of your AI model.

## How does logging work in Baserun?

There are two types of logs in Baserun: **Events** and **Evaluations**.

<Accordion title="Event properties:">
All events have the following properties:

- **event_id:** A unique identifier for the event.
- **session_id:** A unique identifier for the session. For a session, all events have the same session_id.
- **event_type:** The type of event. Can be model or custom.
- **event_name:** The name of the event.Optional for model event but required for custom event.
- **created_at:** The timestamp of the event.
- **config:** The configuration of the event. This can be the model configuration.
- **inputs:** The inputs to the event. This can be the input of the prompt template etc.
- **output:** The output of the event. This can be a completion, an API response, etc.
- **error:** The error message of the event.
- **metadata:** Additional metadata about the event. This can be the product metadata, error metadata, etc.
- **environment:** The source of the event. This can be “production”, “dev”, etc.
- **duration:** The duration of the event. This can be the LLM call duration.
- **end_user_properties (optional):** The user properties of the event.

</Accordion>

### LLM call:

Model events are used to track the execution of your AI model. These can be used to capture:

- Model configuration like model name, model configurations, prompt template, etc.
- Model metrics like completion token count, cost, etc.
- API-level metrics like request latency, rate limit errors, etc.

### Action:

Actions are used to track the execution of anything other than the model such as function calls.

### Custom log:

Custom events are used to capture additional information such as other 3rd party API results or interesting business logic.

## Traces:

A trace comprises a series of events executed within an AI pipeline, enabling Baserun to display the pipeline's entire lifecycle, whether it's synchronous or asynchronous. In this documentation, we often refer to the event within a trace as a 'step'.

A Baserun trace is very similar to a trace in observability frameworks, except that only AI model events and custom events are captured.

## Evaluations

You can programmatically check the a particular event’s output or a trace’s output.

When running an evaluation with Baserun, you can view the evaluation result of an event in the detail panel. You can also see the aggregated results at the top within a test run report or the monitoring dashboard.

Learn more about [Evaluation](/testing/evaluation)

- **Track app performance over time:** Having regression suites or integration suites that run regularly with evaluations can help you understand your application’s performance over time.
- **Debugging & deep analysis:** Helps you understand a particular step within your pipeline, allowing you to pinpoint the step that needs changes quickly.
- **Comparing variations:** Helps you understand the overall performance of different pipeline variations or prompt template variations, so you can pick the version that performs better for your use case.
- **Dataset Curation:** Enables you to curate your test or fine-tuning datasets.
