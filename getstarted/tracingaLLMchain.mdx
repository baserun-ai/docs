---
title: "Tracing a LLM Chain"
sidebarTitle: "Tracing a LLM Chain"
description: "Get started with tracing your LLM Chain with Baserun within 3 mins."
---

## Introduction:

A trace comprises a series of events executed within an LLM chain. Trace enables Baserun to display the LLM chain's entire lifecycle, whether synchronous or asynchronous. In this documentation, we often refer to the event within a trace as a 'step.'

Tracing LLM chains allows you to debug your application, monitor your LLM chains' performance, and collect customer feedback.

## Use case:

Please reference the [Logging Overview](/logging/overview) to learn why logging LLM chain is critical for LLM feature development.

## Features:

- Support Python and Typescript
- Automatically logs OpenAI and Anthropic LLM calls
- UI to show sequence of events
- Provide generative token usage and context token usage
- Provide estimated cost
- Support evaluation
- Provide duration
- Supports async functions
- Support collecting user feedback (Coming soon)

<Note>
  If you are using Next.js, please reference [Logging > Tracing with
  Next.js](/logging/tracingwithnextjs) .
</Note>

The first 3 steps are the same as the [Logging LLM requests tutorial](/). So, if you have already done this before, jump to step 4.

<Steps>
<Step title="Install Baserun SDK">

<CodeGroup>

    ```bash python
    pip install baserun
    ```

    ```bash typescript
    npm install baserun
    # or
    yarn add baserun
    ```

</CodeGroup>
</Step>
<Step title="Generate an API key">

Create an account at [https://baserun.ai](https://baserun.ai). Then generate an API key for your project in the [settings](https://baserun.ai/settings) tab. Set it as an environment variable:

```bash
export BASERUN_API_KEY="your_api_key_here"
```

Or if using python, set `baserun.api_key` to its value:

```python
baserun.api_key = "br-..."
```

</Step>
<Step title=" Initialize">

At your applicationâ€™s startup:

<CodeGroup>
```python python
import baserun

if env == 'production':
baserun.init()

````

```typescript typescript
import { baserun } from 'baserun';

if (process.env.NODE_ENV === 'production') {
    baserun.init();
}
````

</CodeGroup>
</Step>
<Step title="Decide what to trace">

The function(s) to trace are ultimately dependent on your app. It could be a main() function, or it could be a handler for an API call. If you plan to trace different functions, remember to give each trace a name so you can easily filter them later.

<CodeGroup>
```python python
# Decorate the function that you would like to trace:
import baserun

@baserun.trace
def get_response(message):
...

````

```typescript typescript
import { baserun } from 'baserun';

const traceGetResponse = baserun.trace(aiService.getResponse, { userId: 123 });
const response = await traceGetResponse(message);
````

</CodeGroup>
</Step>
</Steps>

Congrats, you are done! Now, you can navigate to the monitoring tab. Here is what you will see interact with your application:

![Trace a LLM chain](/images/logging-llm-requests.png)

Click on the the trace name to see the trace details:

![Trace a LLM chain](/images/trace-detail.png)

Optionally, you can add metadata like trace name, user ID, and session ID to aid in debugging. Read [Logging > Advanced tracing features](/logging/advancedtracing) for more details.

[python example repo](https://github.com/baserun-ai/testing-agent/), <br />
[typescript example repo](https://github.com/baserun-ai/testing-agent-js)

If you have any questions or feature requests, join our [Discord channel](https://discord.com/invite/xEPFsvSmkb) or send us an email at [hello@baserun.ai](mailto:hello@baserun.ai)
